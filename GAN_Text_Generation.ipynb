{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Susrith45/Genie-Gan/blob/main/GAN_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk23T9eQ4fV4"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc_APBha4s3e"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets accelerate evaluate peft bitsandbytes sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvywBXB94zEx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import inspect\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Transformers imports\n",
        "from transformers import (\n",
        "    GPT2TokenizerFast,\n",
        "    GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# ------------- Basic checks -------------\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ------------- Load data -------------\n",
        "# Change path as needed. Must have a 'text' column.\n",
        "csv_path = \"my_dataset.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    raise FileNotFoundError(f\"{csv_path} not found. Upload your CSV with a 'text' column or change path.\")\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"text\" not in df.columns:\n",
        "    raise ValueError(\"CSV must contain a 'text' column. Rename your text column or modify the code.\")\n",
        "\n",
        "texts = df[\"text\"].astype(str).tolist()\n",
        "print(f\"Loaded {len(texts)} rows from {csv_path}\")\n",
        "\n",
        "# ------------- Train / Val split -------------\n",
        "train_texts, val_texts = train_test_split(texts, test_size=0.05, random_state=42)\n",
        "print(f\"Train size: {len(train_texts)}, Val size: {len(val_texts)}\")\n",
        "\n",
        "train_ds = Dataset.from_dict({\"text\": train_texts})\n",
        "val_ds   = Dataset.from_dict({\"text\": val_texts})\n",
        "\n",
        "# ------------- Tokenizer & model -------------\n",
        "MODEL_NAME = \"gpt2\"   # change to gpt2-medium etc. if you have more memory\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "# GPT-2 has no pad token by default â€” set it to eos_token to avoid errors\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "# move model to device for later inference (Trainer handles training device automatically)\n",
        "model.to(device)\n",
        "\n",
        "# ------------- Tokenization -------------\n",
        "max_length = 128   # reduce if you get OOM; increase if you have more memory\n",
        "\n",
        "def tokenize_batch(examples):\n",
        "    return tokenizer(examples[\"text\"],\n",
        "                     truncation=True,\n",
        "                     padding=\"max_length\",\n",
        "                     max_length=max_length)\n",
        "\n",
        "print(\"Tokenizing train dataset...\")\n",
        "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "print(\"Tokenizing val dataset...\")\n",
        "val_tok = val_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# For causal LM, labels = input_ids\n",
        "def add_labels(batch):\n",
        "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
        "    return batch\n",
        "\n",
        "train_tok = train_tok.map(add_labels, batched=True)\n",
        "val_tok   = val_tok.map(add_labels, batched=True)\n",
        "\n",
        "print(train_tok)\n",
        "print(val_tok)\n",
        "\n",
        "# ------------- Data collator -------------\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# ------------- TrainingArguments (robust across versions) -------------\n",
        "# Build a kwargs dict of common args (we will filter to only those supported by the installed TrainingArguments)\n",
        "common_args = dict(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    evaluation_strategy=\"steps\",   # newer versions\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=200,\n",
        "    fp16=torch.cuda.is_available(),   # mixed precision if GPU available\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\", # Explicitly disable reporting to any platform\n",
        ")\n",
        "\n",
        "# Inspect TrainingArguments signature and pass only supported params to avoid TypeError on older transformers\n",
        "sig = inspect.signature(TrainingArguments.__init__)\n",
        "valid_params = set(sig.parameters.keys())\n",
        "\n",
        "# Prepare filtered kwargs\n",
        "filtered_args = {k: v for k, v in common_args.items() if k in valid_params}\n",
        "\n",
        "# Handle very old transformers that use evaluate_during_training instead of evaluation_strategy\n",
        "if \"evaluation_strategy\" not in filtered_args and \"evaluate_during_training\" in valid_params:\n",
        "    # map to old param names if present\n",
        "    filtered_args[\"evaluate_during_training\"] = True\n",
        "    # older versions may not accept eval_steps; drop it if not supported\n",
        "    if \"eval_steps\" in valid_params:\n",
        "        filtered_args[\"eval_steps\"] = common_args[\"eval_steps\"]\n",
        "\n",
        "print(\"TrainingArguments will be created with these keys:\", sorted(filtered_args.keys()))\n",
        "training_args = TrainingArguments(**filtered_args)\n",
        "\n",
        "# ------------- Trainer -------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer created. You can now run trainer.train() to start training.\")\n",
        "# If you want to start training immediately, uncomment the next line:\n",
        "# trainer.train()\n",
        "\n",
        "# ------------- Evaluate utilities -------------\n",
        "# Function to evaluate and print perplexity\n",
        "def evaluate_and_print_ppl(tr):\n",
        "    res = tr.evaluate()\n",
        "    print(\"Evaluation results:\", res)\n",
        "    if \"eval_loss\" in res and res[\"eval_loss\"] is not None:\n",
        "        try:\n",
        "            ppl = math.exp(res[\"eval_loss\"])\n",
        "            print(f\"Perplexity: {ppl:.2f}\")\n",
        "        except OverflowError:\n",
        "            print(\"Perplexity could not be computed from eval_loss (overflow).\")\n",
        "    return res\n",
        "\n",
        "# Example: (uncomment to evaluate now)\n",
        "# evaluate_and_print_ppl(trainer)\n",
        "\n",
        "# ------------- Inference helper -------------\n",
        "def generate_samples(prompt=\"The acting in this movie was\", num_return_sequences=2, max_new_tokens=60):\n",
        "    # Load model & tokenizer from trainer if saved; otherwise use current objects\n",
        "    model.eval()\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    out = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_ids.shape[1] + max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    decoded = [tokenizer.decode(s, skip_special_tokens=True) for s in out]\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"   # Disable wandb logging\n",
        "training_args.num_train_epochs = 1\n",
        "max_length = 64\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "8cWtaDsVJV9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "import math\n",
        "if \"eval_loss\" in eval_results:\n",
        "    ppl = math.exp(eval_results[\"eval_loss\"])\n",
        "    print(f\"Perplexity: {ppl:.2f}\")\n"
      ],
      "metadata": {
        "id": "e1GI3VWdN2lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
        "print(\"Model and tokenizer saved to ./gpt2-finetuned\")\n"
      ],
      "metadata": {
        "id": "4xsE_8M0N7vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt2-finetuned\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
        "\n",
        "prompt = \"The acting in this movie was\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=80,\n",
        "    temperature=0.9,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"\\n=== SAMPLE {i+1} ===\")\n",
        "    print(tokenizer.decode(output, skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqosfDRiODOa",
        "outputId": "bf66892c-4728-466e-80c4-841b350fc3ce"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SAMPLE 1 ===\n",
            "The acting in this movie was made by Robert Rodriguez. The movie opens in theaters on June 21.\n",
            "\n",
            "On June 17, Warner Bros. announced the release date for the second film, in which two characters, a former prostitute who was kidnapped and tortured and murdered, stand by a man who killed him and then went on to commit the crime with his own blood-soaked severed arm. The\n",
            "\n",
            "=== SAMPLE 2 ===\n",
            "The acting in this movie was just me, because I felt like I was the one who made it. There were times I would take the whole time, which really did make it a little easier on myself. It was fun.\"\n",
            "\n",
            "In the short film, he's taking care of a younger son, Ryan, who's struggling in school.\n",
            "\n",
            "\"We're doing the same kind of\n",
            "\n",
            "=== SAMPLE 3 ===\n",
            "The acting in this movie was more than the acting in any other film I've ever seen.\"\n",
            "\n",
            "When asked if she felt like she was being played by the same person from previous films, Darden said: \"I think that's pretty good. In order to do that in this case, it's basically a little bit of casting. You're basically playing one of the characters, and your\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8TIU5i8BnRYdukDMBydoo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}